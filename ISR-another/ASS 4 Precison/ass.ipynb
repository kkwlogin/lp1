{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148a815d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D184', 'D27', 'D373', 'D81', 'D276', 'D95', 'D144', 'D285', 'D322', 'D401']\n",
      "['D184', 'D27', 'D276', 'D285', 'D401']\n",
      "Precision: 0.5\n",
      "Recall: 1.0\n",
      "F-measure: 0.667\n",
      "E-measure: 0.333\n",
      "NDCG: 0.889\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Load Excel dataset\n",
    "df = pd.read_excel(\"cranfield_q1_dataset.xlsx\")\n",
    "\n",
    "# Filter only query q1\n",
    "df_q1 = df[df[\"Query ID\"] == \"q1\"].sort_values(\"Rank\")\n",
    "# Retrieved and relevant sets\n",
    "retrieved = df_q1[\"Document ID\"].tolist()\n",
    "print(retrieved)\n",
    "relevant = (df_q1[df_q1[\"Is_Relevant\"] == \"Yes\"][\"Document ID\"].tolist())\n",
    "print(relevant)\n",
    "\n",
    "# Precision\n",
    "def precision(retrieved, relevant):\n",
    "    if not retrieved: return 0\n",
    "    return sum(doc in relevant for doc in retrieved) / len(retrieved)\n",
    "\n",
    "# Recall\n",
    "def recall(retrieved, relevant):\n",
    "    if not relevant: return 0\n",
    "    return sum(doc in relevant for doc in retrieved) / len(relevant)\n",
    "\n",
    "# F-measure\n",
    "def f_measure(p, r):\n",
    "    if (p + r) == 0: return 0\n",
    "    return 2 * p * r / (p + r)\n",
    "\n",
    "# E-measure\n",
    "def e_measure(p, r, beta=1):\n",
    "    if p == 0 and r == 0: return 1\n",
    "    return 1 - ((1 + beta**2) * p * r) / (beta**2 * p + r)\n",
    "\n",
    "# DCG\n",
    "def dcg(retrieved, relevant):\n",
    "    return sum(1 / math.log2(i+2) for i, doc in enumerate(retrieved) if doc in relevant)\n",
    "\n",
    "# IDCG\n",
    "def idcg(relevant, k):\n",
    "    return sum(1 / math.log2(i+2) for i in range(min(len(relevant), k)))\n",
    "\n",
    "# NDCG\n",
    "def ndcg(retrieved, relevant):\n",
    "    dcg_val = dcg(retrieved, relevant)\n",
    "    idcg_val = idcg(relevant, len(retrieved))\n",
    "    return dcg_val / idcg_val if idcg_val > 0 else 0\n",
    "\n",
    "# ---- Run metrics ----\n",
    "p = precision(retrieved, relevant)\n",
    "r = recall(retrieved, relevant)\n",
    "f = f_measure(p, r)\n",
    "e = e_measure(p, r)\n",
    "n = ndcg(retrieved, relevant)\n",
    "\n",
    "print(\"Precision:\", round(p, 3))\n",
    "print(\"Recall:\", round(r, 3))\n",
    "print(\"F-measure:\", round(f, 3))\n",
    "print(\"E-measure:\", round(e, 3))\n",
    "print(\"NDCG:\", round(n, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90363cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Documents: ['D184', 'D27', 'D373', 'D81', 'D276', 'D95', 'D144', 'D285', 'D322', 'D401']\n",
      "Relevant Documents: ['D184', 'D27', 'D276', 'D285', 'D401']\n",
      "\n",
      "=== Information Retrieval Metrics for Query q1 ===\n",
      "Precision : 0.5\n",
      "Recall    : 1.000\n",
      "F-measure : 0.667\n",
      "E-measure : 0.333\n",
      "NDCG      : 0.889\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# === Step 1: Load your real dataset ===\n",
    "df = pd.read_excel(\"cranfield_q1_dataset.xlsx\")\n",
    "\n",
    "# === Step 2: Filter for Query q1 ===\n",
    "df_q1 = df[df[\"Query ID\"] == \"q1\"].sort_values(\"Rank\")\n",
    "\n",
    "# === Step 3: Extract Retrieved and Relevant Docs ===\n",
    "retrieved = df_q1[\"Document ID\"].tolist()\n",
    "relevant = df_q1[df_q1[\"Is_Relevant\"] == \"Yes\"][\"Document ID\"].tolist()\n",
    "\n",
    "print(\"Retrieved Documents:\", retrieved)\n",
    "print(\"Relevant Documents:\", relevant)\n",
    "\n",
    "# === Step 4: Compute Precision and Recall ===\n",
    "retrieved_relevant = [d for d in retrieved if d in relevant]\n",
    "precision = len(retrieved_relevant) / len(retrieved) if retrieved else 0\n",
    "recall = len(retrieved_relevant) / len(relevant) if relevant else 0\n",
    "\n",
    "# === Step 5: F-measure ===\n",
    "f_measure = (2 * precision * recall / (precision + recall)) if (precision + recall) else 0\n",
    "\n",
    "# === Step 6: E-measure (CORRECTED) ===\n",
    "beta = 1\n",
    "# Standard E-measure formula\n",
    "if precision > 0 and recall > 0:\n",
    "    e_measure = 1 - (1 / ((beta**2 / (1 + beta**2)) * (1/precision) + (1 / (1 + beta**2)) * (1/recall)))\n",
    "else:\n",
    "    e_measure = 1.0  # Maximum error when precision or recall is 0\n",
    "\n",
    "# Alternative simpler E-measure (if beta=1):\n",
    "# e_measure = 1 - f_measure\n",
    "\n",
    "# === Step 7: NDCG Calculation ===\n",
    "df_q1[\"rel_score\"] = df_q1[\"Is_Relevant\"].apply(lambda x: 1 if x == \"Yes\" else 0)\n",
    "\n",
    "rels = df_q1[\"rel_score\"].tolist()  # actual relevance scores list\n",
    "\n",
    "# DCG (actual order)\n",
    "dcg = sum(rels[i] / math.log2(i + 2) for i in range(len(rels)))\n",
    "\n",
    "# IDCG (ideal order)\n",
    "ideal_rels = sorted(rels, reverse=True)\n",
    "idcg = sum(ideal_rels[i] / math.log2(i + 2) for i in range(len(ideal_rels)))\n",
    "ndcg = dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "# === Step 8: Display Results ===\n",
    "print(\"\\n=== Information Retrieval Metrics for Query q1 ===\")\n",
    "print(\"Precision :\",round(precision, 3))\n",
    "print(f\"Recall    : {recall:.3f}\")\n",
    "print(f\"F-measure : {f_measure:.3f}\")\n",
    "print(f\"E-measure : {e_measure:.3f}\")\n",
    "print(f\"NDCG      : {ndcg:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
