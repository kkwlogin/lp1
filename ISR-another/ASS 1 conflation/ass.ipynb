{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1339d5a1",
   "metadata": {},
   "source": [
    "**ASS 1**\n",
    "1.\tImplement Conflation algorithm to generate document representative of a text file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f985249d",
   "metadata": {},
   "source": [
    "run only on python below 3.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc87bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conflation Algorithm: Generate Document Representative of a Text File\n",
    "# Using Stemming and Lemmatization\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "\n",
    "# Download resources (run once)\n",
    "nltk.download('punkt')  \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Step 1: Read input text file\n",
    "filename = \"Conflation.txt\"   # <-- use your own text file\n",
    "with open(filename, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(\"Original Text:\\n\", text)\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Step 2: Tokenization\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# Step 3: Remove punctuation and stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "\n",
    "# Step 4: Apply Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# Step 5: Apply Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Step 6: Display results\n",
    "print(\"After Stemming:\\n\", ' '.join(stemmed_words))\n",
    "print(\"-\" * 80)\n",
    "print(\"After Lemmatization:\\n\", ' '.join(lemmatized_words))\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Step 7: Create Document Representative (word frequency)\n",
    "from collections import Counter\n",
    "freq = Counter(stemmed_words)\n",
    "print(\"Document Representative (Word Frequency):\")\n",
    "for word, count in freq.most_common(10):\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5e666d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " Information retrieval is the process of obtaining information from large collections of text.\n",
      "It involves searching, indexing, and ranking documents based on user queries.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "filename = \"Conflation.txt\"   # <-- use your own text file\n",
    "with open(filename, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(\"Original Text:\\n\", text)\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "481d2b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " Information retrieval is the process of obtaining information from large collections of text.\n",
      "It involves searching, indexing, and ranking documents based on user queries.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Read input text file\n",
    "filename = \"Conflation.txt\"   # <-- use your own text file\n",
    "text = open(filename, 'r', encoding='utf-8').read()\n",
    "\n",
    "print(\"Original Text:\\n\", text)\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc65f6b7",
   "metadata": {},
   "source": [
    "Imports (lines 1–6)\n",
    "\n",
    "import nltk\n",
    "\n",
    "Loads the NLTK package. Needed to access tokenizers, corpora, and NLP utilities. Without this, nothing NLTK-related will run.\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "Imports the stopwords corpus module. This provides predefined lists of frequent words (like \"the\", \"is\") that you usually remove during text processing because they carry little semantic content.\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "Imports word_tokenize, a standard tokenizer that splits raw text into a list of tokens (words and punctuation). It handles punctuation and common tokenization edge cases better than str.split().\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "Imports two algorithms:\n",
    "\n",
    "PorterStemmer: reduces words to a stem (heuristic chopping) — fast but may produce non-dictionary stems.\n",
    "\n",
    "WordNetLemmatizer: reduces words to dictionary lemmas using WordNet (needs correct POS for best results; default assumes noun).\n",
    "\n",
    "import string\n",
    "\n",
    "Imports Python’s string module (useful for punctuation lists, though in your code you use word.isalpha() instead of string.punctuation).\n",
    "\n",
    "Why separate imports? — keeps code readable and loads only the functions you need.\n",
    "\n",
    "NLTK downloads (lines 8–11)\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "Downloads the Punkt tokenizer models used by word_tokenize. Run once per environment. If already present, this call returns quickly.\n",
    "\n",
    "nltk.download('punkt_tab') (nonstandard)\n",
    "\n",
    "Note: punkt_tab is not a standard NLTK resource. This line likely came from a trial or a mistaken suggestion. It may produce a \"Resource not found\" error or be ignored. You can safely remove this line — punkt alone is enough for tokenization.\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "Downloads the stopword lists (English, etc.). Needed for stopwords.words('english').\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "Downloads the WordNet lexical database required by WordNetLemmatizer.\n",
    "\n",
    "Important: Download calls typically print messages and may prompt in some environments — for automated runs you might prefer to ensure these resources exist beforehand.\n",
    "\n",
    "File read (lines 13–16)\n",
    "\n",
    "filename = \"Conflation.txt\"\n",
    "\n",
    "Defines the filename string. The file must be in the same directory where the script runs, or provide an absolute path.\n",
    "\n",
    "with open(filename, 'r', encoding='utf-8') as file:\n",
    "\n",
    "Opens the file in read mode with UTF-8 encoding. Using with ensures the file is closed automatically even on error.\n",
    "\n",
    "text = file.read()\n",
    "\n",
    "Reads the entire file into one string variable text. For very large files you might stream line-by-line, but for typical assignments this is fine.\n",
    "\n",
    "Why encoding='utf-8'? — To handle non-ASCII characters safely; avoids UnicodeDecodeError for many text files.\n",
    "\n",
    "Debugging print + separator (lines 18–19)\n",
    "\n",
    "print(\"Original Text:\\n\", text)\n",
    "\n",
    "Prints the raw content — useful for verifying input and demonstrating results to the examiner.\n",
    "\n",
    "print(\"-\" * 80)\n",
    "\n",
    "Prints a visual separator (80 dashes) to make console output easier to read.\n",
    "\n",
    "Tokenization (line 22)\n",
    "\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "Converts the entire text to lowercase (text.lower()) to normalize case (so “Apple” and “apple” are treated same).\n",
    "\n",
    "word_tokenize() splits the text into tokens. Tokens include words and punctuation (e.g., [\"the\", \"runner\", \",\", \"he\"]).\n",
    "\n",
    "Why lowercase first? — Case normalization reduces vocabulary size and improves conflation results.\n",
    "\n",
    "Stopword and punctuation removal (lines 25–27)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "Loads the English stopwords and converts to a set for O(1) membership checks (faster than list).\n",
    "\n",
    "tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "\n",
    "This list comprehension filters tokens:\n",
    "\n",
    "word.isalpha() → keeps only tokens consisting of alphabetic characters (removes punctuation and numeric tokens).\n",
    "\n",
    "Effect: tokens like 'don't' becomes 'don' and 't' if you tokenized before; isalpha() removes tokens containing apostrophes or hyphens. If you need to preserve contractions, consider a different filter.\n",
    "\n",
    "word not in stop_words → removes common stopwords.\n",
    "\n",
    "Result: tokens becomes a list of cleaned, lowercased, alphabetic words with no stopwords.\n",
    "\n",
    "Pitfall: isalpha() will remove words containing apostrophes (e.g., \"don't\"), hyphens, or accents. If your text has those, preprocessing must be customized.\n",
    "\n",
    "Stemming (lines 30–31)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "Creates an instance of PorterStemmer. You call methods on this object to stem words.\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "Applies stemming to every token, producing a new list stemmed_words.\n",
    "\n",
    "Stemming behavior: reduces inflections and derivations by chopping endings (e.g., running → run, happier → happier may become happier or happi depending on algorithm).\n",
    "\n",
    "Note: Stems are not guaranteed to be real words (may be truncated). This is acceptable for bag-of-words style document representatives.\n",
    "\n",
    "Lemmatization (lines 34–35)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "Creates a lemmatizer object that uses WordNet to return dictionary lemmas.\n",
    "\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "Lemmatizes each token. By default, lemmatize assumes the word is a noun. For better results, you would supply part-of-speech tags (e.g., 'v' for verb) — otherwise running remains running unless specified as a verb.\n",
    "\n",
    "Difference vs stemming: Lemmatization returns valid dictionary words when given correct POS; it's more linguistically correct but requires POS or may underperform.\n",
    "\n",
    "Display results (lines 38–43)\n",
    "\n",
    "print(\"After Stemming:\\n\", ' '.join(stemmed_words))\n",
    "\n",
    "Joins stemmed words with spaces and prints them as a single line. This shows the effect of stemming across the document.\n",
    "\n",
    "print(\"-\" * 80)\n",
    "\n",
    "Separator.\n",
    "\n",
    "print(\"After Lemmatization:\\n\", ' '.join(lemmatized_words))\n",
    "\n",
    "Joins and prints lemmatized tokens for comparison with stems.\n",
    "\n",
    "print(\"-\" * 80)\n",
    "\n",
    "Separator.\n",
    "\n",
    "Why show both? — Teachers like to see both outputs to judge understanding: stemming (algorithmic) vs lemmatization (semantic).\n",
    "\n",
    "Document representative (frequency) (lines 46–50)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "Imports Counter for counting frequencies of items in a list (efficient and concise).\n",
    "\n",
    "freq = Counter(stemmed_words)\n",
    "\n",
    "Creates a frequency map (stem → count) using the stemmed tokens. Using stems in the representative merges word forms (e.g., run, running, ran → same stem), which demonstrates conflation.\n",
    "\n",
    "print(\"Document Representative (Word Frequency):\")\n",
    "\n",
    "Prints a header for the output.\n",
    "\n",
    "for word, count in freq.most_common(10):\n",
    "\n",
    "Iterates over the top 10 most frequent stems in descending order.\n",
    "\n",
    "print(f\"{word}: {count}\")\n",
    "\n",
    "Prints each stem and its frequency count.\n",
    "\n",
    "Why use stems for the representative? — Because conflation aims to treat related forms as one feature; using stems reduces dimensionality and makes the representative more meaningful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
