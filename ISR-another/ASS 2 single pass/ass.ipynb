{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f481451c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file1.txt created successfully.\n",
      "file2.txt created successfully.\n",
      "file3.txt created successfully.\n",
      "file4.txt created successfully.\n",
      "file5.txt created successfully.\n",
      "\n",
      "All 5 files created in current directory ✅\n"
     ]
    }
   ],
   "source": [
    "# === Create Example Text Files for Single-Pass Clustering ===\n",
    "\n",
    "texts = {\n",
    "    \"file1.txt\": \"Machine learning is a branch of artificial intelligence.\",\n",
    "    \"file2.txt\": \"Artificial intelligence and machine learning are related fields.\",\n",
    "    \"file3.txt\": \"Football is the most popular sport in the world.\",\n",
    "    \"file4.txt\": \"Cricket and football are played all over the world.\",\n",
    "    \"file5.txt\": \"Python is used for data science and machine learning.\"\n",
    "}\n",
    "\n",
    "for filename, content in texts.items():\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "        print(f\"{filename} created successfully.\")\n",
    "\n",
    "print(\"\\nAll 5 files created in current directory ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "900d75cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1: ['file1.txt', 'file2.txt']\n",
      "Cluster 2: ['file3.txt', 'file4.txt']\n",
      "Cluster 3: ['file5.txt']\n"
     ]
    }
   ],
   "source": [
    "# === Single-Pass Clustering Algorithm for Text Documents ===\n",
    "# CO4 - Distributed and Multimedia Information Retrieval\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# --- Step 1: Mention the files manually ---\n",
    "filenames = [\"file1.txt\", \"file2.txt\", \"file3.txt\", \"file4.txt\", \"file5.txt\"]\n",
    "\n",
    "# --- Step 2: Read file contents ---\n",
    "documents = [open(f, \"r\", encoding=\"utf-8\").read() for f in filenames]\n",
    "\n",
    "# --- Step 3: Convert to TF-IDF vectors ---\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf = vectorizer.fit_transform(documents)\n",
    "\n",
    "# --- Step 4: Set similarity threshold ---\n",
    "THRESHOLD = 0.3\n",
    "\n",
    "# --- Step 5: Single-Pass Clustering ---\n",
    "clusters, centroids = [], []\n",
    "\n",
    "for i, vec in enumerate(tfidf):\n",
    "    assigned = False\n",
    "    for j, c in enumerate(centroids):\n",
    "        sim = cosine_similarity(vec, c)[0][0]\n",
    "        if sim >= THRESHOLD:\n",
    "            clusters[j].append(filenames[i])\n",
    "            centroids[j] = (centroids[j] + vec) / 2\n",
    "            assigned = True\n",
    "            break\n",
    "    if not assigned:\n",
    "        clusters.append([filenames[i]])\n",
    "        centroids.append(vec)\n",
    "\n",
    "# --- Step 6: Display results ---\n",
    "for i, c in enumerate(clusters, 1):\n",
    "    print(f\"Cluster {i}: {c}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb322626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1: ['file1.txt', 'file2.txt']\n",
      "Cluster 2: ['file3.txt']\n",
      "Cluster 3: ['file4.txt']\n",
      "Cluster 4: ['file5.txt']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' “Go through each document one by one,\\nand get both its index (i) and its TF-IDF vector (vec).”'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer       # To convert text into TF-IDF vectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity             # To compute cosine similarity between documents\n",
    "\n",
    "# --- Step 1: Mention the text files manually ---\n",
    "filenames = [\"file1.txt\", \"file2.txt\", \"file3.txt\", \"file4.txt\", \"file5.txt\"]  # List of text files to cluster\n",
    "\n",
    "# --- Step 2: Read file contents into a list ---\n",
    "documents = [open(f, \"r\", encoding=\"utf-8\").read() for f in filenames]         # Read each file's content\n",
    "\n",
    "# --- Step 3: Convert the documents into TF-IDF vectors ---\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")   # Create TF-IDF vectorizer (removes common English words)\n",
    "tfidf = vectorizer.fit_transform(documents)          # Generate TF-IDF matrix (numeric representation of text)\n",
    "\n",
    "# --- Step 4: Set similarity threshold ---\n",
    "THRESHOLD = 0.4                                     # If similarity ≥ threshold, same cluster; else new cluster\n",
    "\n",
    "# --- Step 5: Initialize empty lists for clusters and centroids ---\n",
    "clusters, centroids = [], []                         # clusters -> list of lists; centroids -> average vectors\n",
    "\n",
    "# --- Step 6: Single-Pass Clustering process ---\n",
    "for i, vec in enumerate(tfidf):                      # For each document vector\n",
    "    assigned = False                                 # Flag to check if assigned to any existing cluster\n",
    "\n",
    "    for j, c in enumerate(centroids):                # Compare with each cluster centroid\n",
    "        sim = cosine_similarity(vec, c)[0][0]        # Compute cosine similarity between document and centroid\n",
    "        if sim >= THRESHOLD:                         # If similarity above threshold → same cluster\n",
    "            clusters[j].append(filenames[i])          # Add this file to that cluster\n",
    "            centroids[j] = (centroids[j] + vec) / 2   # Update cluster centroid (average of all members)\n",
    "            assigned = True                           # Mark as assigned\n",
    "            break                                     # Exit inner loop once assigned\n",
    "\n",
    "    if not assigned:                                 # If not similar to any existing cluster\n",
    "        clusters.append([filenames[i]])               # Create a new cluster with this file\n",
    "        centroids.append(vec)                         # Set this file's vector as new cluster centroid\n",
    "\n",
    "# --- Step 7: Display final clusters ---\n",
    "for i, c in enumerate(clusters, 1):                  # Loop through all formed clusters\n",
    "    print(f\"Cluster {i}: {c}\")                       # Print cluster number and the file names inside it\n",
    "    \n",
    "    \n",
    "\"\"\" “Go through each document one by one,\n",
    "and get both its index (i) and its TF-IDF vector (vec).”\"\"\"   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
